-Learned about CHOMP
	-Read paper on CHOMP
	-Talked with Matt about his paper

-Reoriented myself with ROS and begin to learn about the Moveit! software
	-Watched a few video tutorials

-Examined the Fetch tutorial:
	-Learned how to open rviz and Gazebo
	-Teleoped the robot around the playground environment
	-Learned how to manipulate Fetch's environment by hand (not programatically)
	-Checked out the demo code provided for traveling around the playground workspace
	-Looked and played with the arm manipulation demo code
	-Briefly checked out the head camera topics and the api documents
	-Delved into the demo code and tried to see how ROS was making it work-the comments on the 		 demo code can be seen in the cappy_demo file
	-Created my own version of the demo code-made the robot pick up a block from other table

-Examined how to create our own environment for the robot:
	-Explored the Gazebo website
	-Learned about how to insert Gazebo's models into the simulation environment
	-Figured out how to insert a mesh (colloda file or sdf file) into an environment
	-Looked at Google's 3D warehouse and the Sketchup software
	-Made a few different building models using Gazebo's building editor and integrated one of 		 the newly created buildings into a world with Gazebo's premade models-the instructions for 		 this are enumerated in the fetch_gazebo folder in the NewWorldInstructions text file

-Learned how to build a map for the robot so that it can navigate around a newly created environment
	-Used the map_server to save a newly created map
	-Integrated newly created map into the launch files-instructions for this can be found in the 		 fetch_navigation folder in a text file called InstructionsForMakingANewMap
	-Navigated robot around new environment

-Checked to see if the robot could pick up an object more complicated than a cube or sphere
	-Tried to make robot pick up a bowl
	-Looked at the ROS household objects database to try and find grasps for complicated objects

-Tested to see how robust the map of the environment is
	-Found that adding an object to be picked up that was not previously on the map is not a 		 problem-the robot will see it when you add a solid primitive to the scene
	-Found that moving the surface the object is on is only a problem if you move the object to 		 be in the way of the normal path of the robot
	-Found that when adding something like a cone to the scene the robot could get around the 		 cone with little to no problems-sometimes the robot would minorly bump the cone
	-These results can be seen in the fetch_gazebo folder in the notes text file

-Oriented myself to the mechanics of the real robot
	-Read through the tutorials
	-Attempted to callibrate the robot
	-Used the joystick to navigate the robot around the room
	-Created a map of the lab by navigating the robot around the room with the joystick
	-Attempted to move the arm using rviz but something strange happened and the robot had to be 		 shut down
	-Navigated the robot around the room using code instead of the joystick

-Learned how to plan and view a trajectory and how to then execute said trajectory
	-Learned more about the rviz MotionPlanning interface
	-The PickPlaceInterface() and MoveGroupInterface() both have a plan_only function
	-Used the result of the planned arm movement in the FollowTrajectoryClient from the original 		 demo Gazebo code
	-Created an interface that would allow the user to choose whether they wanted to execute the 		 trajectory, quit the trajectory, or replan the trajectory
	-This also works when specifying joint states for the robot instead of telling the robot to 		 pick up or place a specific object
	
-Learned more about rviz and how to visualize sensor information
	-Examined the ROS octomap representation
	-Learned how to integrate a sensor file for the robot
	-Removed the MotionPlanning interface from rviz and replaced it with the RobotModel, Planning 		-Scene, and Trajectory interfaces
	-Learned that if you set allow_active_sensing to 'True' then the robot will avoid obstacles 		 in its path that it can see with its camera
	-Made the simulated robot lift its arm and wave where the wave functionality was executed 		 using only the FollowTrajectoryClient()
	-Made a system that allowed us to check to see that the robot's actual last pose corresponded 		 to its planned last pose (i.e. did the robot actually finish executing the trajectory that 		 we think it did?)
	-Learned about the robot state publisher

-Made progress on planning and executing a trajectory in rviz
	-Streamlined the code that previews trajectories for pick and place goals, joint goals, and 		 pose goals
	-Tested the real robot's ability to perceive objects that aren't cubes, spheres, or cylinders
	-Found that the robot was able to find grasps for more complicated objects; for example the 		 robot was able to find grasps for the joystick that controls the robot
	
-Ran pickup on actual robot
	-Initially, robot executed trajectory, barely missing the table, but it missed the object 		 that we were trying to make it pick up and then hit the table when it put its arm down to 		 try and pick up the object
	-We found that there was a calibration error and realized that Gretchen was miscalculating 		 where her own arm was
	-Upon calibrating the robot we had far fewer promblems with picking up and placing objects

-As a result of poor Moveit! execution we looked into the inverse kinematics of the robot
	-In Gazebo examined the different planners included in the package that we received with 		 Fetch-the results of this exploration can be found in planners.txt
	-Looked at padding the support surface so that the robot is less likely to hit it because it 		 seems as if the planners that are better still get too close to obstacles
	-Found other ways to make planning safer in ROS' PlanningOptions message
	-Tested different planners using the actual robot and made the script run more smoothly
	-Learned that there seems to still be an offset error that shows up between where we want the 		 arm to go and where it actually ends up-as a result we added an execution tracker

-Began to work on getting the robot to press the down elevator button
	-Wrote a script using OpenCV's HoughCircles function in order to detect circles in an image
	-Brought Gretchen out to elevator but robot lab wifi didn't extend out far enough
	-Learned how to interface OpenCV with ROS using cvbridge
	-Learned about rosbag which allowed us to record video from the robot and then run cvbridge 		 on the recorded and saved video
	-Found that the red rubber ball was a more reliable object to use when testing the techniques 		 for finding circles
	-Found image_geometry interface which allowed me to find the image point in the camera frame
	-Tested the points from the image frame to see whether they seemed to be in the correct 	 place-for the most part they matched up but it was hard to tell where the camera frame was 		 initiating from
	-Used TfListener.transformPoint and TfListener.waitForTransform to get point in the base_link 		 frame
	-Drew a circle on a block of wood and attempted to make Gretchen tap the center of the circle 		 using the moveToPose() method from the MoveGroupInterface()
	-Found that there was a problem where Gretchen wouldn't want to touch the block because this 		 would cause a collision and Gretchen wouldn't be able to come up with a plan
	-Tried to scale down the size of the gripper links so that Gretchen would collide with the 		 block but on our initial run the arm moved in erratic ways
	-Investigated trying to move Gretchen's end effector 3 more cm using inverse kinematics 	 instead of ROS' default planners
	-Acheived this goal but ran into problems in the retreat from the button-sometimes the roll 		 links would make part of the arm rotate 360 degrees as it moved back
	-To fix this problem we allowed Gretchen to tuck directly from pushing the button

-Made Gretchen pick the block up from the floor and place the block in the trash can
	-After this we tried to integrate much of what we'd been working on this summer and used the 		 torso controller, the head controller, the client that moves the base and the arm controller
	-We found that perhaps because of a poorly constructed map Gretchen had a few significant 		 difficulties trying to move around the room-we were therefore only successful in raising 		 Gretchen's torso, tilting her head down, and picking up the block
	-Created new map to adjust for the changes that have been made to the room









